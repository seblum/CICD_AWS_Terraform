About CI/CD
CI/CD stands for “Continuous Integration / Continuous Delivery” and is a key concept of DevOps. At its most basic level, CI/CD Pipelines are just scripts which are triggered whenever code is changed (e.g. as the result of a push or merge), automatically building software assets, running tests and deploying code.

There is an important distinction between CI/CD Pipelines and Training Pipelines. CI/CD Pipelines run in response to code changes, whereas training Training Pipelines are often run on demand or on a schedule unrelated to code changes.

CI/CD Pipelines and Workflows are highly mature in the Software Engineering world, but are under-utilised in the world of Machine Learning and Data Science.

About Merge / Pull Requests & CI/CD
Pull/Merge requests are requests for newly developed code to be integrate it into “trunk” of the code base. A Merge / Pull request is generated by the engineer (or data scientist), requesting that the team accept the new changes to the code base, which will often require the new code pass a series of gates, including:

The change relates back to a change request / ticket (e.g. Jira Task)
Unit tests pass
Automated integration tests pass
User acceptance testing
Code review
Once the code has passed these gates (many of which will be automated), the code is ready to be merged into the code base for deployment.

We will use the concept of Merge Requests as the gates for human intervention in our deployment process, ensuring that each deployment is signed off by a real person. We have found that manual, “human in the loop” sign-off on deployments is much more palatable for senior management from a Data Governance perspective than purely automated solutions.

CI/CD for Machine Learning using Kubeflow
Within a Machine Learning workflow, there are two kinds of deployments:

Deploying code (Pipeline code, Inference code)
Deploying models (e.g. .joblib / encoding information)
Traditionally, only code is stored within a Git repository, with data being stored in a Database, Data Lake or Data Warehouse. Machine Learning models more closely resemble “data” than “code”, and are not really suitable for storage in a Git repository for the following reasons:

Model files are machine generated, not the work of humans
Models are often very large (potentially in the GB)
Joblib files have a binary format, which means they do not make sense to diff and changes are difficult to reason about
However, it does make sense to store a “link” to where we can find a model, so that we can rollback to previous versions, or to link the current version of code with the current version of a model. This is especially true since changes in code may mean an older model is no longer compatible. The link can be managed via git-lfs, or by a simple json file pointing to where the file exists in shared storage (e.g. S3, Hadoop, Google Cloud Storage or Azure Blob Storage).

Storing a reference to the current version of the model (along with a checksum for verification), means we can also use the same Merge / Pull Request pattern to deploy models as we would to deploy code. This also means that we can have a “Model Review” phase of our deployment workflow, where a human can decide (& take responsibility) for the deployment of model to production

